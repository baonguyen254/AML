{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML-Lab08",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLGmFg7QxPSa"
      },
      "source": [
        "**LAB 08: REFORCEMENT LEARNING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPUcCnkgxfm6"
      },
      "source": [
        "1. Tìm hiểu về OpenAI\r\n",
        "2. Chơi thử trò SmartCar\r\n",
        "3. Làm quen với Naive và Q-Learning\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dAqTNYhx146"
      },
      "source": [
        "* OpenAI là 1 công cụ được tạo ra nhằm giúp những nhà nghiên cứu dễ dàng hơn trong việc có 1 benchmark tốt bằng cách tạo một môi trường ổn định, có cách cài đặt đơn giản. Mục đích của công cụ này là giúp tăng khả năng reproduce lại các kết quả trong lĩnh vực AI, cũng như cung cấp 1 công cụ giúp chúng ta dễ dàng thao tác với các môi trường AI hơn. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKt5phSoAG0Q"
      },
      "source": [
        "#Cài đặt thư viện \r\n",
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFluN8jFASrO"
      },
      "source": [
        "Chúng ta sẽ nói về một environment đơn giản, có số state và số action hữu hạn (và khá nhỏ) là Taxi-v2. Trong environment này, agent của chúng ta đóng vai trò 1 tài xế taxi. Có 4 địa điểm cố định khác nhau trên bản đồ (được ký hiệu R, G, Y, B), và mỗi khi environment bắt đầu, sẽ có 2 điểm bất kỳ là điểm đón và trả khách (2 điểm này có thể trùng nhau), cũng như vị trí của taxi cũng là vị trí bất kỳ. Nhiệm vụ của chúng ta là đón hành khách (ở điểm màu xanh da trời) và trả khách (ở điểm màu tím).\r\n",
        "\r\n",
        "Agent của chúng ta có thể thực hiện 6 actions:\r\n",
        "\r\n",
        "0: xuống dưới\r\n",
        "\r\n",
        "1: lên trên\r\n",
        "\r\n",
        "2: sang trái\r\n",
        "\r\n",
        "3: sang phải\r\n",
        "\r\n",
        "4: đón khách\r\n",
        "\r\n",
        "5: trả khách\r\n",
        "\r\n",
        "Reward của environment này được tính như sau:\r\n",
        "\r\n",
        "cứ sau 1 time step (tức là khi xảy ra bất kỳ action nào), sẽ nhận -1 reward\r\n",
        "nhận được +20 reward nếu ta trả khách thành công (nghĩa là time step đó ta sẽ nhận +19 reward)\r\n",
        "nếu agent có hành vi đón khách và trả khách không hợp lệ, nhận -10 reward (nghĩa là time step đó ta sẽ nhận -11 reward).\r\n",
        "\r\n",
        "taxi sẽ hoạt động trên một khu vực 5x5, với 4 điểm trả khác và 5 địa điểm hành khách đang đứng nên số states sẽ là 5x5x5x4=500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkAJ7c3wCB_j"
      },
      "source": [
        "import numpy as np\r\n",
        "import gym\r\n",
        "import time\r\n",
        "from IPython.display import clear_output\r\n",
        "env = gym.make(\"Taxi-v3\") #Gọi môi trường ra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK4uhv_ZAvar",
        "outputId": "b20f8e7c-125d-401c-c9cb-ec7502553d79"
      },
      "source": [
        "env.env.s=env.encode(1,1,2,0)\r\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9shg8mBWA2-h"
      },
      "source": [
        "Hành khách có thể ở 5 điểm là R, G, Y, B và trong xe, màu xanh dương để chỉ địa điểm hành khách đang đứng chờ (nếu hành khác trong taxi thì xe sẽ màu xanh lá), màu tím để chỉ địa điểm cần chở khách đến"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0ByBeA1BfsL",
        "outputId": "92a282f2-f329-4817-8e5d-79fce453da1f"
      },
      "source": [
        "env.env.s=env.encode(1,1,4,0)\r\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO3XzAmVBrAa"
      },
      "source": [
        "ta sẽ dùng env.encode(số_hàng,số_cột,hành_khách,điểm_trả_khách), với:\r\n",
        "* số_hàng thuộc [0,1,2,3,4]\r\n",
        "* số_cột thuộc  [0,1,2,3,4]\r\n",
        "* hành_khách: 0 là ở R, 1 là ở G, 2 là ở Y, 3 là ở B, 4 là trên xe\r\n",
        "* điểm_trả_khách: 0 là ở R, 1 là ở G, 2 là ở Y, 3 là ở B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8aRFBl4EF1J"
      },
      "source": [
        "#Naive-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w-poHgtddc3"
      },
      "source": [
        "Bây giờ ta sẽ học Q-table một cách ngây thơ :\r\n",
        "\r\n",
        "* Chọn hành động tốt nhất dựa vào những gì đã học\r\n",
        "* Nếu điểm bằng nhau thì chọn ngẫu nhiên\r\n",
        "* Q-table cập nhất với discount = 0 (không xem xét tương lai)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWLkda4pETSu"
      },
      "source": [
        "Đầu tiên ta phải tạo ra một q-table là ma trận $size\\_state\\times size\\_action$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ2W9KYoD2--"
      },
      "source": [
        "state_size=env.observation_space.n # số state\r\n",
        "action_size=env.action_space.n # số action\r\n",
        "q_table=np.zeros((state_size,action_size))\r\n",
        "\r\n",
        "FILE_SAVE = \"q_table.npy\" # định nghĩa file để lưu q_table\r\n",
        "total_episodes= 1000 # tổng số episodes\r\n",
        "total_test_episodes = 100     # tổng số episodes để test\r\n",
        "max_steps = 99                # số steps để dừng cho 1 episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFMwLsuyc2k3"
      },
      "source": [
        "state_size = env.observation_space.n\r\n",
        "action_size = env.action_space.n\r\n",
        "\r\n",
        "q_table = np.zeros((state_size, action_size))\r\n",
        "\r\n",
        "FILE_SAVE_Naive = \"q_table_naive.npy\"\r\n",
        "total_episodes = 5000       # Total episodes\r\n",
        "total_test_episodes = 100     # Total test episodes\r\n",
        "max_steps = 99                # Max steps per episode\r\n",
        "           # Exponential decay rate for exploration prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2IWSA-Iara-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGOCGE3ec83M",
        "outputId": "8b9a9496-92e7-4da6-960b-b100f056f903"
      },
      "source": [
        "import time\r\n",
        "for episode in range(total_episodes):\r\n",
        "    state = env.reset()   # reset lại môi trường\r\n",
        "    done = False          # đã hoàn thành trả khách hay chưa\r\n",
        "    for step in range(max_steps):\r\n",
        "        clear_output(wait=True)\r\n",
        "        print(\"episode: \",episode)\r\n",
        "        if  np.max(q_table[state]) ==0:         # nếu trong state q_value đều bằng 0 thì chọn đại 1 action\r\n",
        "            action=np.random.randint(0,action_size)\r\n",
        "        else:\r\n",
        "            action = np.argmax(q_table[state])  # chọn action có q_value lớn nhất\r\n",
        "\r\n",
        "        new_state, reward, done, _ = env.step(action)   # thực hiện hành động để nhận reward và state, action mới\r\n",
        "\r\n",
        "        q_table[state,action] += reward # cập nhật q_table\r\n",
        "        state = new_state\r\n",
        "        if done:       # nếu taxi có hành vi trả khách thì kết thúc episode\r\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode:  4999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moGEPh_fMeTc"
      },
      "source": [
        "# lưu lại q_table\r\n",
        "np.save(FILE_SAVE_Naive,q_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "js8wTRHUMejA",
        "outputId": "d9bfbccd-bc8d-4a14-c0d8-2fa58acc1d2c"
      },
      "source": [
        "# từ q_table bắt đầu chơi thử\r\n",
        "q_table=np.load(\"q_table_naive.npy\")\r\n",
        "rewards = []\r\n",
        "\r\n",
        "for episode in range(total_test_episodes):\r\n",
        "    state = env.reset()\r\n",
        "    step = 0\r\n",
        "    done = False\r\n",
        "    total_rewards = 0\r\n",
        "    \r\n",
        "\r\n",
        "    for step in range(max_steps):\r\n",
        "        print(\"****************************************************\")\r\n",
        "        print(\"EPISODE \", episode)\r\n",
        "        action = np.argmax(q_table[state,:])\r\n",
        "        \r\n",
        "        new_state, reward, done, info = env.step(action)\r\n",
        "        env.render() # hiển thị trò chơi\r\n",
        "        total_rewards += reward\r\n",
        "        print(total_rewards)\r\n",
        "        time.sleep(0.5)\r\n",
        "        clear_output(wait=True)\r\n",
        "        \r\n",
        "        if done:\r\n",
        "            rewards.append(total_rewards)\r\n",
        "            #print (\"Score\", total_rewards)\r\n",
        "            break\r\n",
        "        state = new_state\r\n",
        "env.close()\r\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  41\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
            "+---------+\n",
            "  (South)\n",
            "-74\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-04ec51c28685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SYLNclrdVJn"
      },
      "source": [
        "# Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGXLLkkBdYYT"
      },
      "source": [
        "Giá trị Q-value được cập nhật như sau:$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha [r_{t+1} + \\lambda \\max_{a}Q(s_{t+1}, a) - Q(s_t, a_t)]$$với $\\alpha$ là learning rate, $\\lambda$ là discount rate, $s_t$ là quan sát thời điểm $t$ và $r_{t+1}$ là phần thưởng sau khi thực hiện hành động $a_t$ với quan sát $s_t$.\r\n",
        "\r\n",
        "Ngoài ra agent sẽ ngẫu nhiên thực hiện exploration với xác suất nào đó ở những state ban đầu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvs2PFgGo2Bt"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\")\r\n",
        "state_size = env.observation_space.n\r\n",
        "action_size = env.action_space.n\r\n",
        "\r\n",
        "q_table = np.zeros((state_size, action_size))\r\n",
        "\r\n",
        "FILE_SAVE_Qlearning = \"q_table_qlearning.npy\"\r\n",
        "total_episodes = 5000         # Total episodes\r\n",
        "total_test_episodes = 100     # Total test episodes\r\n",
        "max_steps = 99                # Max steps per episode\r\n",
        "\r\n",
        "learning_rate = 0.7           # Learning rate\r\n",
        "discount_rate = 0.95         # Discounting rate\r\n",
        "\r\n",
        "# Exploration parameters\r\n",
        "epsilon = 1.0                 # Exploration rate\r\n",
        "max_epsilon = 1.0             # Exploration probability at start\r\n",
        "min_epsilon = 0.01            # Minimum exploration probability \r\n",
        "decay_rate = 0.01             # Exponential decay rate for exploration prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqAMf8YYevFC",
        "outputId": "d9eb36da-2067-4e5a-acd6-583240cbc708"
      },
      "source": [
        "import time\r\n",
        "for episode in range(total_episodes):\r\n",
        "    state = env.reset()\r\n",
        "    done = False\r\n",
        "    for step in range(max_steps):\r\n",
        "        clear_output(wait=True)\r\n",
        "        print(\"episode: \",episode)\r\n",
        "        epsilon = min(min_epsilon, epsilon*decay_rate)\r\n",
        "        # kiểm tra xem agent dùng exploi hay explor\r\n",
        "        if np.random.rand() < epsilon:\r\n",
        "            # exploration\r\n",
        "            action = np.random.randint(0, action_size)\r\n",
        "        else:\r\n",
        "            # exploitation\r\n",
        "            if np.max(q_table[state])==0:\r\n",
        "                action=np.random.randint(0,action_size)\r\n",
        "            else:\r\n",
        "                action = np.argmax(q_table[state])\r\n",
        "            \r\n",
        "        # nhận reward và state tiếp theo\r\n",
        "        new_state, reward, done, _ = env.step(action)\r\n",
        "\r\n",
        "        # cập nhật q_table theo Bellman equation\r\n",
        "        update = reward + discount_rate*q_table[new_state].max() - q_table[state,action]\r\n",
        "        q_table[state,action] = q_table[state,action] + learning_rate*update\r\n",
        "        state = new_state\r\n",
        "        if done:\r\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode:  4999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKiSW-mMf_Wd"
      },
      "source": [
        "# lưu lại q_table\r\n",
        "np.save(FILE_SAVE_Qlearning,q_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pvzBTLvgL8P",
        "outputId": "8d906bc7-b93d-42ba-9785-46713888aa71"
      },
      "source": [
        "# từ q_table bắt đầu chơi thử\r\n",
        "q_table=np.load(\"q_table_qlearning.npy\")\r\n",
        "rewards = []\r\n",
        "\r\n",
        "for episode in range(total_test_episodes):\r\n",
        "    state = env.reset()\r\n",
        "    step = 0\r\n",
        "    done = False\r\n",
        "    total_rewards = 0\r\n",
        "    \r\n",
        "\r\n",
        "    for step in range(max_steps):\r\n",
        "        print(\"****************************************************\")\r\n",
        "        print(\"EPISODE \", episode)\r\n",
        "        \r\n",
        "        action = np.argmax(q_table[state,:])\r\n",
        "        new_state, reward, done, info = env.step(action)\r\n",
        "        env.render() # hiển thị trò chơi\r\n",
        "        total_rewards += reward\r\n",
        "        print(total_rewards)\r\n",
        "        time.sleep(0.5)\r\n",
        "        clear_output(wait=True)\r\n",
        "        \r\n",
        "        if done:\r\n",
        "            rewards.append(total_rewards)\r\n",
        "            #print (\"Score\", total_rewards)\r\n",
        "            break\r\n",
        "        state = new_state\r\n",
        "        \r\n",
        "env.close()\r\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 8.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqLhCQdVltdm"
      },
      "source": [
        "#Bài tập"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc29B4SZlvxt"
      },
      "source": [
        "1. Giải thích vì sao khi dùng Naive-Learning thì xe taxi chỉ đứng yên một chỗ?\r\n",
        "2. Giải thích vì sao khi dùng Q-Leaning thì xe taxi có thể đón và trả khách được?\r\n",
        "3. Tìm hiểu một game khác trên OpenAI và thiết lập cho agent chơi được"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmztcQ5vlTTQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}